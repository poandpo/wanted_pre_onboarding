{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "원티드 프리온보딩 코스pynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN7l0MUlfWImyb6kmPjW0tG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poandpo/wanted_pre_onboarding/blob/main/%EC%9B%90%ED%8B%B0%EB%93%9C_%ED%94%84%EB%A6%AC%EC%98%A8%EB%B3%B4%EB%94%A9_%EC%BD%94%EC%8A%A4pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 과제"
      ],
      "metadata": {
        "id": "9Q6_yAQSeIEr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OtdgDaD2ILYg"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "    for i in range(len(sequences)):\n",
        "      tok = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", sequences[i]) #영어,숫자 제외 제거\n",
        "      low = tok.lower().split() # 소문자로 변환하고 분리\n",
        "      result.append(low)\n",
        "\n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    #전처리 함수 후 N차원리스트를 단순 리스트로 변환\n",
        "    pre = sum(self.preprocessing(sequences), []) \n",
        "    # 중복 제외하곤 self.word_dict에 {단어: 숫자} 추가\n",
        "    for word in pre:  \n",
        "      if word not in self.word_dict:\n",
        "         self.word_dict[word] = len(self.word_dict)\n",
        "    self.fit_checker = True\n",
        "\n",
        "  '''\n",
        "  transform\n",
        "  1) 토큰화 된 문장들--> 각각 문장들로 구분\n",
        "  2-1) 문장의 단어들--> 단어(key)의 value(정수)로 바꿈\n",
        "  2-2) 단어의 value가 존재하지 않을시 oov로 간주 --> oov 인덱스로 바뀜\n",
        "  2-3) value들이 sub_re로 추가\n",
        "  3) 한 문장이 다 value로 바뀌어 하나의 sub_re가 완성되면 sub_re를 result에 추가\n",
        "  '''\n",
        "\n",
        "  def transform(self, sequences):\n",
        "    result = []\n",
        "    tokens = self.preprocessing(sequences)\n",
        "    if self.fit_checker:\n",
        "      for token in tokens : #(1)\n",
        "        sub_re =[]\n",
        "        for word in token:\n",
        "           try:\n",
        "             sub_re.append(self.word_dict[word]) #(2-1)\n",
        "           except KeyError:\n",
        "             sub_re.append(self.word_dict['oov'])  #(2-2)\n",
        "        result.append(sub_re) #(2-3)\n",
        "      return result #(3)\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "\n",
        "\n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log \n",
        "\n",
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False    \n",
        "\n",
        "    '''\n",
        "    fit\n",
        "    1) 토큰화하여서 나온 nested list--> 1차원 리스트로 변경 ex) [1,2,3,4]\n",
        "    2) df(d,f): 각각 토큰들이 문장마다 있는지 --> 있다면 +1 \n",
        "    3) idf식에 대입한 값을 result에 추가    \n",
        "    '''\n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    self.IDF_Matrix = []\n",
        "    self.ts =  list(set(sum(tokenized, []))) # 인덱싱된 정수(토큰)들을 중복제거 후 1차원 리스트로 \n",
        "    n = len(tokenized) # 총 문서\n",
        "    for i in range(len(self.ts)): \n",
        "      t = self.ts[i]\n",
        "      df = 0\n",
        "      for token in tokenized: # (2)\n",
        "        df +=  t in token\n",
        "      self.IDF_Matrix.append(log(n/(df+1))) #(3)\n",
        "    self.fit_checker = True\n",
        "\n",
        "    '''\n",
        "    transform\n",
        "    1)TF행렬 \n",
        "    1-1) 각각 문장으로 구분\n",
        "    1-2) output_type을 위한 []추가 \n",
        "    1-3) 특정 문서안에 특정 토큰의 갯수 TF 행렬에 추가\n",
        "        cf) TF_matrix[-1] --> [-1]하지 않을시 output type 에 맞출수 없음 ex) [[],0,[],1]\n",
        "    2)TF-IDF행렬: tf(d,f) X idf(d,f)\n",
        "    2-1) tf(d,f): 특정 문장에서 특정 문자가 갯수() \n",
        "        idf(d,f): self.IDF_Matrix[x]\n",
        "    2-2) 위 결과를 TF-IDF 행렬 추가 \n",
        "    '''\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "       tokenized = self.tokenizer.transform(sequences)\n",
        "       n =len(tokenized)\n",
        "       self.tfidf_matrix =[]\n",
        "        # TF_행렬\n",
        "       TF_matrix = []\n",
        "       for i in range(n):  \n",
        "         TF_matrix.append([])       \n",
        "         self.tfidf_matrix.append([])\n",
        "         d = tokenized[i] #특정 문서\n",
        "         for x in range(len(self.ts)): #self.ts : 전체 토큰이 있는 리스트\n",
        "           t = self.ts[x]  # 특정 토큰 \n",
        "           TF_matrix[-1].append(d.count(t)) \n",
        "\n",
        "          # TF-IDF 행렬\n",
        "           tfidf= d.count(t) * self.IDF_Matrix[x] \n",
        "           self.tfidf_matrix[-1].append(tfidf)\n",
        "       return self.tfidf_matrix\n",
        "    else:\n",
        "       raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "efR68pFibC5c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test"
      ],
      "metadata": {
        "id": "aPGdCgmYeE0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "li=  ['I go to school.', 'I LIKE pizza!']\n",
        "\n",
        "TOK= Tokenizer()\n",
        "TOK.fit(li)"
      ],
      "metadata": {
        "id": "e_fCoiLHddfb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOK.transform(li)"
      ],
      "metadata": {
        "id": "iSaQfETSc6oj",
        "outputId": "cb3079a0-9588-48e5-f16c-054039e0e817",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4], [1, 5, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "li2=  ['I go to school.', 'I LIKE pizza!', 'chae!!']\n",
        "\n",
        "TOK= Tokenizer()\n",
        "TOK.fit_transform(li2)"
      ],
      "metadata": {
        "id": "ff_mWrZEc9Kg",
        "outputId": "9f2d63a9-7cbe-4ec2-dcb6-01b30cc4b262",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4], [1, 5, 6], [7]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "li=  ['I go to school.', 'I LIKE pizza!']\n",
        "\n",
        "\n",
        "TF=TfidfVectorizer(TOK)\n",
        "TF.fit(li)"
      ],
      "metadata": {
        "id": "GFMU1viCdhjr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TF.transform(li)"
      ],
      "metadata": {
        "id": "V38VQN9PdtVT",
        "outputId": "97e44805-17eb-4f25-8b69-227ab235ca45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[-0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
              " [-0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TF.fit_transform(li2)"
      ],
      "metadata": {
        "id": "cOnm2WrJdyO0",
        "outputId": "b138e561-0f24-4ca4-e5f0-e91305cb5f4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0,\n",
              "  0.4054651081081644,\n",
              "  0.4054651081081644,\n",
              "  0.4054651081081644,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.0],\n",
              " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}